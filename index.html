<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding">
  
  <meta property="og:title" content="VRU-Accident"/>
  <meta property="og:description" content=" Multimodal Large Language Models, Accident-Centric Benchmark, Vulnerable Road Users(VRUs), Autonomous Driving"/>
  <meta property="og:url" content="https://vru-accident.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->

  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multimodal Large Language Models, Accident-Centric Benchmark, Vulnerable Road Users(VRUs), Autonomous Driving">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://kimyounggun99.github.io/" target="_blank">Younggun Kim</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=1sZuPRQAAAAJ&hl=en" target="_blank">Ahmed S. Abdelrahman</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=Cp4F4JkAAAAJ&hl=en" target="_blank">Mohamed Abdel-Aty</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Central Florida</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper(Coming Soon)</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link 
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary(Coming Soon)</span>
                    </a>
                  </span>

                  -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/Kimyounggun99/VRU-Accident" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://huggingface.co/datasets/kyh9191/VRU-Accident" target="_blank"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Benchmark</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv(Coming Soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/VRU_Accident_Video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Vulnerable Road Users (VRUs), such as pedestrians and cyclists, face high risk in traffic environments, making it essential to understand the causes and preventability of accidents involving them. Against this backdrop, Multimodal Large Language Models (MLLMs) have shown promise in complex scene understanding for applications like autonomous driving and accident analysis. However, there is no benchmark specifically designed to evaluate MLLMs in safety-critical situations involving Vulnerable Road Users (VRUs), where fine-grained reasoning, causal inference, and prevention-focused understanding are essential. We present VRU-Accident, the first vision-language benchmark that focuses on real-world traffic accidents involving VRUs, supporting both video question answering and dense captioning to assess MLLMsâ€™ capabilities in accident scene understanding.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p> We present VRU-Accident, a large-scale vision-language benchmark designed to evaluate multimodal large language models (MLLMs) in high-risk traffic scenarios involving Vulnerable Road Users (VRUs) such as pedestrians and cyclists. VRU-Accident comprises 1K real-world dashcam accident videos, annotated with 6K multiple-choice question-answer pairs across six safety-critical categories (with 24K candidate options and 3.4K unique answer choices), as well as 1K dense scene descriptions. Unlike prior works, our benchmark focuses explicitly on VRU-vehicle accidents, providing rich, fine-grained annotations that capture both spatial-temporal dynamics and causal semantics of accidents. To assess the current landscape of MLLMs, we conduct a comprehensive evaluation of 17 state-of-the-art models on the multiple-choice VQA task and on the dense captioning task. Our findings reveal that while MLLMs perform reasonably well on visually grounded attributes, they face significant challenges in reasoning and describing accident causes, types, and preventability.

          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->




<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">VQA Statistics</h2>

      <div class="content has-text-centered">
        <img src="static/images/VQA_Statistics.png" alt="Example 1"/>
        <br><br><br>
        <img src="static/images/VQA_Word_frequencies.png" alt="Example 2"/>
      </div>

    </div>
  </div>
</section>



<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Performance of MLLMs on the VQA Task</h2>

      <div class="content has-text-centered">
        <img src="static/images/VQA_results.png" alt="VQA result"/>

      </div>

    </div>
  </div>
</section>





<!-- Image  -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Qualitative Examples of VQA and Dense Captioning tasks on VRU-Accident Benchmark</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
       
        <img src="static/images/All_model_examples/Example_Visualization_Part1.png" alt="MY ALT TEXT"/>

      </div>
      <div class="item">
       
        <img src="static/images/All_model_examples/Example_Visualization_Part2.png" alt="MY ALT TEXT"/>

      </div>
      <div class="item">
     
        <img src="static/images/All_model_examples/Example_Visualization_Part3.png" alt="MY ALT TEXT"/>

       </div>
       <div class="item">
      
        <img src="static/images/All_model_examples/Example_Visualization_Part4.png" alt="MY ALT TEXT"/>

      </div>


      <div class="item">
       
        <img src="static/images/All_model_examples/Example_Visualization_Part5.png" alt="MY ALT TEXT"/>

      </div>
      <div class="item">
       
        <img src="static/images/All_model_examples/Example_Visualization_Part6.png" alt="MY ALT TEXT"/>

      </div>
      <div class="item">
     
        <img src="static/images/All_model_examples/Example_Visualization_Part7.png" alt="MY ALT TEXT"/>

     </div>
     <div class="item">
     
      <img src="static/images/All_model_examples/Example_Visualization_Part8.png" alt="MY ALT TEXT"/>

    </div>



  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- End video carousel -->





<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
